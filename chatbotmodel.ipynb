{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chatbotmodel.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["! pip install konlpy\n","import konlpy\n","from konlpy.tag import Okt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQZ3AdO2KAKq","outputId":"26e48dd6-3294-4af4-c79c-5ae1fd16b898"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 7.3 MB/s \n","\u001b[?25hCollecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 45.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["import torch\n","import os\n","import re\n","import json\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","#from konlpy.tag import Okt"],"metadata":{"id":"xTpvxDIYKIBL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","from __future__ import unicode_literals\n","\n","import torch\n","from torch.jit import script, trace\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import csv\n","import random\n","import re\n","import os\n","import unicodedata\n","import codecs\n","from io import open\n","import itertools\n","import math\n","\n","\n","USE_CUDA = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"],"metadata":{"id":"m-76GfHmKPMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_name = \"chatbot.txt\"\n","corpus = os.path.join(\"/content/drive/MyDrive/chatcorpus/\")"],"metadata":{"id":"3sOMWWDnkmV2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datafile = os.path.join(corpus,corpus_name)"],"metadata":{"id":"OfswUMVHlsBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def printLines(file, n=10):\n","    with open(file, 'r') as datafile:\n","        lines = datafile.readlines()\n","    for line in lines[:n]:\n","        print(line)"],"metadata":{"id":"vyxFZk0soqen"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["printLines(datafile)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EbmfuOTLorbq","outputId":"511ab4dc-92d1-4358-f682-51eca5705cfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12시 땡!\t하루가 또 가네요.\n","\n","1지망 학교 떨어졌어\t위로해 드립니다.\n","\n","3박4일 놀러가고 싶다\t여행은 언제나 좋죠.\n","\n","3박4일 정도 놀러가고 싶다\t여행은 언제나 좋죠.\n","\n","PPL 심하네\t눈살이 찌푸려지죠.\n","\n","SD카드 망가졌어\t다시 새로 사는 게 마음 편해요.\n","\n","SD카드 안돼\t다시 새로 사는 게 마음 편해요.\n","\n","SNS 맞팔 왜 안하지ㅠㅠ\t잘 모르고 있을 수도 있어요.\n","\n","SNS 시간낭비인 거 아는데 매일 하는 중\t시간을 정하고 해보세요.\n","\n","SNS 시간낭비인데 자꾸 보게됨\t시간을 정하고 해보세요.\n","\n"]}]},{"cell_type":"code","source":["r = open(datafile, mode='r', encoding='utf-8')"],"metadata":{"id":"v8EoNfwRpmWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["r.readlines()"],"metadata":{"id":"c-YPy_BXp14O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 기본 단어 토큰 값\n","PAD_token = 0  # 짧은 문장을 채울(패딩, PADding) 때 사용할 제로 토큰\n","SOS_token = 1  # 문장의 시작(SOS, Start Of Sentence)을 나타내는 토큰\n","EOS_token = 2  # 문장의 끝(EOS, End Of Sentence)을 나태는 토큰\n","\n","class Voc:\n","    def __init__(self, name):\n","        self.name = name\n","        self.trimmed = False\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","        self.num_words = 3  # SOS, EOS, PAD를 센 것\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.num_words\n","            self.word2count[word] = 1\n","            self.index2word[self.num_words] = word\n","            self.num_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","    # 등장 횟수가 기준 이하인 단어를 정리합니다\n","    def trim(self, min_count=0):\n","        if self.trimmed:\n","            return\n","        self.trimmed = True\n","\n","        keep_words = []\n","\n","        for k, v in self.word2count.items():\n","            if v >= min_count:\n","                keep_words.append(k)\n","\n","        print('keep_words {} / {} = {:.4f}'.format(\n","            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n","        ))\n","\n","        # 사전을 다시 초기화힙니다\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","        self.num_words = 3 # 기본 토큰을 센 것\n","\n","        for word in keep_words:\n","            self.addWord(word)"],"metadata":{"id":"U-wqwSByjqpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_LENGTH = 10  # 고려할 문장의 최대 길이\n","\n","# 유니코드 문자열을 아스키로 변환합니다\n","# https://stackoverflow.com/a/518232/2809427 참고\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","# 소문자로 만들고, 공백을 넣고, 알파벳 외의 글자를 제거합니다\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    s = re.sub(r\"\\s+\", r\" \", s).strip()\n","    return s\n","\n","# 질의/응답 쌍을 읽어서 voc 객체를 반환합니다\n","def readVocs(datafile, corpus_name):\n","    print(\"Reading lines...\")\n","    # 파일을 읽고, 쪼개어 lines에 저장합니다\n","    lines = open(datafile, encoding='utf-8').\\\n","        read().strip().split('\\n')\n","    # 각 줄을 쪼개어 pairs에 저장하고 정규화합니다\n","    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n","    voc = Voc(corpus_name)\n","    return voc, pairs\n","\n","# 문장의 쌍 'p'에 포함된 두 문장이 모두 MAX_LENGTH라는 기준보다 짧은지를 반환합니다\n","def filterPair(p):\n","    # EOS 토큰을 위해 입력 시퀀스의 마지막 단어를 보존해야 합니다\n","    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n","\n","# 조건식 filterPair에 따라 pairs를 필터링합니다\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]\n","\n","# 앞에서 정의한 함수를 이용하여 만든 voc 객체와 리스트 pairs를 반환합니다\n","def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n","    print(\"Start preparing training data ...\")\n","    voc, pairs = readVocs(datafile, corpus_name)\n","    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        voc.addSentence(pair[0])\n","        voc.addSentence(pair[1])\n","    print(\"Counted words:\", voc.num_words)\n","    return voc, pairs\n","\n","\n","# voc와 pairs를 읽고 재구성합니다\n","save_dir = os.path.join(\"/content/drive/MyDrive\", \"save\")\n","voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n","# 검증을 위해 pairs의 일부 내용을 출력해 봅니다\n","print(\"\\npairs:\")\n","for pair in pairs[:10]:\n","    print(pair)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L8A9h2LdkDiR","outputId":"27e9de12-0f60-470b-fb63-35df00bcced3","executionInfo":{"status":"ok","timestamp":1649247968991,"user_tz":-540,"elapsed":982,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Start preparing training data ...\n","Reading lines...\n","Read 11823 sentence pairs\n","Trimmed to 11453 sentence pairs\n","Counting words...\n","Counted words: 20070\n","\n","pairs:\n","['12시 땡 !', '하루가 또 가네요 .']\n","['1지망 학교 떨어졌어', '위로해 드립니다 .']\n","['3박4일 놀러가고 싶다', '여행은 언제나 좋죠 .']\n","['3박4일 정도 놀러가고 싶다', '여행은 언제나 좋죠 .']\n","['ppl 심하네', '눈살이 찌푸려지죠 .']\n","['sd카드 망가졌어', '다시 새로 사는 게 마음 편해요 .']\n","['sd카드 안돼', '다시 새로 사는 게 마음 편해요 .']\n","['sns 맞팔 왜 안하지ㅠㅠ', '잘 모르고 있을 수도 있어요 .']\n","['sns 시간낭비인 거 아는데 매일 하는 중', '시간을 정하고 해보세요 .']\n","['sns 시간낭비인데 자꾸 보게됨', '시간을 정하고 해보세요 .']\n"]}]},{"cell_type":"code","source":["MIN_COUNT = 2    # 제외할 단어의 기준이 되는 등장 횟수\n","\n","def trimRareWords(voc, pairs, MIN_COUNT):\n","    # MIN_COUNT 미만으로 사용된 단어는 voc에서 제외합니다\n","    voc.trim(MIN_COUNT)\n","    # 제외할 단어가 포함된 경우를 pairs에서도 제외합니다\n","    keep_pairs = []\n","    for pair in pairs:\n","        input_sentence = pair[0]\n","        output_sentence = pair[1]\n","        keep_input = True\n","        keep_output = True\n","        # 입력 문장을 검사합니다\n","        for word in input_sentence.split(' '):\n","            if word not in voc.word2index:\n","                keep_input = False\n","                break\n","        # 출력 문장을 검사합니다\n","        for word in output_sentence.split(' '):\n","            if word not in voc.word2index:\n","                keep_output = False\n","                break\n","\n","        # 입출력 문장에 제외하기로 한 단어를 포함하지 않는 경우만을 남겨둡니다\n","        if keep_input and keep_output:\n","            keep_pairs.append(pair)\n","\n","    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n","    return keep_pairs\n","\n","\n","# voc와 pairs를 정돈합니다\n","pairs = trimRareWords(voc, pairs, MIN_COUNT)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8p8BNNrfkJ6A","outputId":"4393da71-185a-482b-ae45-879d20de9f3a","executionInfo":{"status":"ok","timestamp":1649247976661,"user_tz":-540,"elapsed":371,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["keep_words 9157 / 20067 = 0.4563\n","Trimmed from 11453 pairs to 4521, 0.3947 of total\n"]}]},{"cell_type":"code","source":["def indexesFromSentence(voc, sentence):\n","    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n","\n","\n","def zeroPadding(l, fillvalue=PAD_token):\n","    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n","\n","def binaryMatrix(l, value=PAD_token):\n","    m = []\n","    for i, seq in enumerate(l):\n","        m.append([])\n","        for token in seq:\n","            if token == PAD_token:\n","                m[i].append(0)\n","            else:\n","                m[i].append(1)\n","    return m\n","\n","# 입력 시퀀스 텐서에 패딩한 결과와 lengths를 반환합니다\n","def inputVar(l, voc):\n","    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n","    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n","    padList = zeroPadding(indexes_batch)\n","    padVar = torch.LongTensor(padList)\n","    return padVar, lengths\n","\n","# 패딩한 목표 시퀀스 텐서, 패딩 마스크, 그리고 최대 목표 길이를 반환합니다\n","def outputVar(l, voc):\n","    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n","    max_target_len = max([len(indexes) for indexes in indexes_batch])\n","    padList = zeroPadding(indexes_batch)\n","    mask = binaryMatrix(padList)\n","    mask = torch.ByteTensor(mask)\n","    padVar = torch.LongTensor(padList)\n","    return padVar, mask, max_target_len\n","\n","# 입력 배치를 이루는 쌍에 대한 모든 아이템을 반환합니다\n","def batch2TrainData(voc, pair_batch):\n","    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n","    input_batch, output_batch = [], []\n","    for pair in pair_batch:\n","        input_batch.append(pair[0])\n","        output_batch.append(pair[1])\n","    inp, lengths = inputVar(input_batch, voc)\n","    output, mask, max_target_len = outputVar(output_batch, voc)\n","    return inp, lengths, output, mask, max_target_len\n","\n","\n","# 검증용 예시\n","small_batch_size = 5\n","batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n","input_variable, lengths, target_variable, mask, max_target_len = batches\n","\n","print(\"input_variable:\", input_variable)\n","print(\"lengths:\", lengths)\n","print(\"target_variable:\", target_variable)\n","print(\"mask:\", mask)\n","print(\"max_target_len:\", max_target_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0tnxpY07rRN4","outputId":"e714de54-10ea-4389-f2bd-e1cecc99b197","executionInfo":{"status":"ok","timestamp":1649247980640,"user_tz":-540,"elapsed":362,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["input_variable: tensor([[4734, 4698, 2235, 3387,  761],\n","        [ 252,  280, 1057, 2078,  774],\n","        [1643, 4699,  636,    2,    2],\n","        [1388, 1461,  111,    0,    0],\n","        [   2,    2,    2,    0,    0]])\n","lengths: tensor([5, 5, 5, 3, 3])\n","target_variable: tensor([[4734, 1443, 2236, 1513,   56],\n","        [1481,  347, 2237,  249,  775],\n","        [4158,    7, 2238, 1287,  628],\n","        [4745,    2,   57,    7,    7],\n","        [   7,    0,    7,    2,    2],\n","        [   2,    0,    2,    0,    0]])\n","mask: tensor([[1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1],\n","        [1, 0, 1, 1, 1],\n","        [1, 0, 1, 0, 0]], dtype=torch.uint8)\n","max_target_len: 6\n"]}]},{"cell_type":"code","source":["class EncoderRNN(nn.Module):\n","    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n","        super(EncoderRNN, self).__init__()\n","        self.n_layers = n_layers\n","        self.hidden_size = hidden_size\n","        self.embedding = embedding\n","\n","        # GRU를 초기화합니다. input_size와 hidden_size 매개변수는 둘 다 'hidden_size'로\n","        # 둡니다. 이는 우리 입력의 크기가 hideen_size 만큼의 피처를 갖는 단어 임베딩이기\n","        # 때문입니다.\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n","                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n","\n","    def forward(self, input_seq, input_lengths, hidden=None):\n","        # 단어 인덱스를 임베딩으로 변환합니다\n","        embedded = self.embedding(input_seq)\n","        # RNN 모듈을 위한 패딩된 배치 시퀀스를 패킹합니다\n","        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n","        # GRU로 포워드 패스를 수행합니다\n","        outputs, hidden = self.gru(packed, hidden)\n","        # 패딩을 언패킹합니다\n","        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n","        # 양방향 GRU의 출력을 합산합니다\n","        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n","        # 출력과 마지막 은닉 상태를 반환합니다\n","        return outputs, hidden"],"metadata":{"id":"4dKUBrForTo_","executionInfo":{"status":"ok","timestamp":1649247983374,"user_tz":-540,"elapsed":371,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# Luong 어텐션 레이어\n","class Attn(nn.Module):\n","    def __init__(self, method, hidden_size):\n","        super(Attn, self).__init__()\n","        self.method = method\n","        if self.method not in ['dot', 'general', 'concat']:\n","            raise ValueError(self.method, \"is not an appropriate attention method.\")\n","        self.hidden_size = hidden_size\n","        if self.method == 'general':\n","            self.attn = nn.Linear(self.hidden_size, hidden_size)\n","        elif self.method == 'concat':\n","            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n","            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n","\n","    def dot_score(self, hidden, encoder_output):\n","        return torch.sum(hidden * encoder_output, dim=2)\n","\n","    def general_score(self, hidden, encoder_output):\n","        energy = self.attn(encoder_output)\n","        return torch.sum(hidden * energy, dim=2)\n","\n","    def concat_score(self, hidden, encoder_output):\n","        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n","        return torch.sum(self.v * energy, dim=2)\n","\n","    def forward(self, hidden, encoder_outputs):\n","        # Attention 가중치(에너지)를 제안된 방법에 따라 계산합니다\n","        if self.method == 'general':\n","            attn_energies = self.general_score(hidden, encoder_outputs)\n","        elif self.method == 'concat':\n","            attn_energies = self.concat_score(hidden, encoder_outputs)\n","        elif self.method == 'dot':\n","            attn_energies = self.dot_score(hidden, encoder_outputs)\n","\n","        # max_length와 batch_size의 차원을 뒤집습니다\n","        attn_energies = attn_energies.t()\n","\n","        # 정규화된 softmax 확률 점수를 반환합니다 (차원을 늘려서)\n","        return F.softmax(attn_energies, dim=1).unsqueeze(1)"],"metadata":{"id":"9QcB_PVArWRn","executionInfo":{"status":"ok","timestamp":1649247985270,"user_tz":-540,"elapsed":2,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["class LuongAttnDecoderRNN(nn.Module):\n","    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n","        super(LuongAttnDecoderRNN, self).__init__()\n","\n","        # 참조를 보존해 둡니다\n","        self.attn_model = attn_model\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","\n","        # 레이어를 정의합니다\n","        self.embedding = embedding\n","        self.embedding_dropout = nn.Dropout(dropout)\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n","        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","        self.attn = Attn(attn_model, hidden_size)\n","\n","    def forward(self, input_step, last_hidden, encoder_outputs):\n","        # 주의: 한 단위 시간에 대해 한 단계(단어)만을 수행합니다\n","        # 현재의 입력 단어에 대한 임베딩을 구합니다\n","        embedded = self.embedding(input_step)\n","        embedded = self.embedding_dropout(embedded)\n","        # 무방향 GRU로 포워드 패스를 수행합니다\n","        rnn_output, hidden = self.gru(embedded, last_hidden)\n","        # 현재의 GRU 출력을 바탕으로 어텐션 가중치를 계산합니다\n","        attn_weights = self.attn(rnn_output, encoder_outputs)\n","        # 인코더 출력에 어텐션을 곱하여 새로운 \"가중치 합\" 문맥 벡터를 구합니다\n","        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n","        # Luong의 논문에 나온 식 5를 이용하여 가중치 문맥 벡터와 GRU 출력을 결합합니다\n","        rnn_output = rnn_output.squeeze(0)\n","        context = context.squeeze(1)\n","        concat_input = torch.cat((rnn_output, context), 1)\n","        concat_output = torch.tanh(self.concat(concat_input))\n","        # Luong의 논문에 나온 식 6을 이용하여 다음 단어를 예측합니다\n","        output = self.out(concat_output)\n","        output = F.softmax(output, dim=1)\n","        # 출력과 마지막 은닉 상태를 반환합니다\n","        return output, hidden"],"metadata":{"id":"z9QRHZyKrX4x","executionInfo":{"status":"ok","timestamp":1649247986651,"user_tz":-540,"elapsed":422,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["def maskNLLLoss(inp, target, mask):\n","    nTotal = mask.sum()\n","    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n","    loss = crossEntropy.masked_select(mask).mean()\n","    loss = loss.to(device)\n","    return loss, nTotal.item()"],"metadata":{"id":"mQ066derrZIA","executionInfo":{"status":"ok","timestamp":1649247986652,"user_tz":-540,"elapsed":2,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n","          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n","\n","    # 제로 그라디언트\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    # device 옵션을 설정합니다\n","    input_variable = input_variable.to(device)\n","    target_variable = target_variable.to(device)\n","    mask = mask.to(device)\n","    # Lengths for rnn packing should always be on the cpu\n","    lengths = lengths.to(\"cpu\")\n","\n","    # 변수를 초기화합니다\n","    loss = 0\n","    print_losses = []\n","    n_totals = 0\n","\n","    # 인코더로 포워드 패스를 수행합니다\n","    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n","\n","    # 초기 디코더 입력을 생성합니다(각 문장을 SOS 토큰으로 시작합니다)\n","    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n","    decoder_input = decoder_input.to(device)\n","\n","    # 디코더의 초기 은닉 상태를 인코더의 마지막 은닉 상태로 둡니다\n","    decoder_hidden = encoder_hidden[:decoder.n_layers]\n","\n","    # 이번 반복에서 teacher forcing을 사용할지를 결정합니다\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    # 배치 시퀀스를 한 번에 하나씩 디코더로 포워드 패스합니다\n","    if use_teacher_forcing:\n","        for t in range(max_target_len):\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs\n","            )\n","            # Teacher forcing 사용: 다음 입력을 현재의 목표로 둡니다\n","            decoder_input = target_variable[t].view(1, -1)\n","            # 손실을 계산하고 누적합니다\n","            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n","            loss += mask_loss\n","            print_losses.append(mask_loss.item() * nTotal)\n","            n_totals += nTotal\n","    else:\n","        for t in range(max_target_len):\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs\n","            )\n","            # Teacher forcing 미사용: 다음 입력을 디코더의 출력으로 둡니다\n","            _, topi = decoder_output.topk(1)\n","            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n","            decoder_input = decoder_input.to(device)\n","            # 손실을 계산하고 누적합니다\n","            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n","            loss += mask_loss\n","            print_losses.append(mask_loss.item() * nTotal)\n","            n_totals += nTotal\n","\n","    # 역전파를 수행합니다\n","    loss.backward()\n","\n","    # 그라디언트 클리핑: 그라디언트를 제자리에서 수정합니다\n","    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n","    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n","\n","    # 모델의 가중치를 수정합니다\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return sum(print_losses) / n_totals"],"metadata":{"id":"iPetEZmmrbIw","executionInfo":{"status":"ok","timestamp":1649247987064,"user_tz":-540,"elapsed":1,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n","\n","    # 각 단계에 대한 배치를 읽어옵니다\n","    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n","                      for _ in range(n_iteration)]\n","\n","    # 초기화\n","    print('Initializing ...')\n","    start_iteration = 1\n","    print_loss = 0\n","    if loadFilename:\n","        start_iteration = checkpoint['iteration'] + 1\n","\n","    # 학습 루프\n","    print(\"Training...\")\n","    for iteration in range(start_iteration, n_iteration + 1):\n","        training_batch = training_batches[iteration - 1]\n","        # 배치에서 각 필드를 읽어옵니다\n","        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n","\n","        # 배치에 대해 학습을 한 단계 진행합니다\n","        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n","                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n","        print_loss += loss\n","\n","        # 경과를 출력합니다\n","        if iteration % print_every == 0:\n","            print_loss_avg = print_loss / print_every\n","            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n","            print_loss = 0\n","\n","        # Checkpoint를 저장합니다\n","        if (iteration % save_every == 0):\n","            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n","            if not os.path.exists(directory):\n","                os.makedirs(directory)\n","            torch.save({\n","                'iteration': iteration,\n","                'en': encoder.state_dict(),\n","                'de': decoder.state_dict(),\n","                'en_opt': encoder_optimizer.state_dict(),\n","                'de_opt': decoder_optimizer.state_dict(),\n","                'loss': loss,\n","                'voc_dict': voc.__dict__,\n","                'embedding': embedding.state_dict()\n","            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"],"metadata":{"id":"T7c6KApgrfwk","executionInfo":{"status":"ok","timestamp":1649247989281,"user_tz":-540,"elapsed":341,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["class GreedySearchDecoder(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(GreedySearchDecoder, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, input_seq, input_length, max_length):\n","        # 인코더 모델로 입력을 포워드 패스합니다\n","        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n","        # 인코더의 마지막 은닉 레이어가 디코더의 첫 번째 은닉 레이어의 입력이 되도록 준비합니다\n","        decoder_hidden = encoder_hidden[:decoder.n_layers]\n","        # 디코더의 첫 번째 입력을 SOS_token으로 초기화합니다\n","        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n","        # 디코더가 단어를 덧붙여 나갈 텐서를 초기화합니다\n","        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n","        all_scores = torch.zeros([0], device=device)\n","        # 반복적으로 각 단계마다 하나의 단어 토큰을 디코딩합니다\n","        for _ in range(max_length):\n","            # 디코더로의 포워드 패스를 수행합니다\n","            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n","            # 가장 가능성 높은 단어 토큰과 그 softmax 점수를 구합니다\n","            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n","            # 토큰과 점수를 기록합니다\n","            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n","            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n","            # 현재의 토큰을 디코더의 다음 입력으로 준비시킵니다(차원을 증가시켜서)\n","            decoder_input = torch.unsqueeze(decoder_input, 0)\n","        # 단어 토큰과 점수를 모아서 반환합니다\n","        return all_tokens, all_scores"],"metadata":{"id":"q2K7CjXurh3C","executionInfo":{"status":"ok","timestamp":1649247991612,"user_tz":-540,"elapsed":1,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n","    ### 입력 시퀀스를 배치 형태로 만듭니다\n","    # 단어 -> 인덱스\n","    indexes_batch = [indexesFromSentence(voc, sentence)]\n","    # lengths 텐서를 만듭니다\n","    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n","    # 배치의 차원을 뒤집어서 모델이 사용하는 형태로 만듭니다\n","    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n","    # 적절한 디바이스를 사용합니다\n","    input_batch = input_batch.to(device)\n","    lengths = lengths.to(\"cpu\")\n","    # searcher를 이용하여 문장을 디코딩합니다\n","    tokens, scores = searcher(input_batch, lengths, max_length)\n","    # 인덱스 -> 단어\n","    decoded_words = [voc.index2word[token.item()] for token in tokens]\n","    return decoded_words\n","\n","\n","def evaluateInput(encoder, decoder, searcher, voc):\n","    input_sentence = ''\n","    while(1):\n","        try:\n","            # 입력 문장을 받아옵니다\n","            input_sentence = input('> ')\n","            # 종료 조건인지 검사합니다\n","            if input_sentence == 'q' or input_sentence == 'quit': break\n","            # 문장을 정규화합니다\n","            input_sentence = normalizeString(input_sentence)\n","            # 문장을 평가합니다\n","            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n","            # 응답 문장을 형식에 맞춰 출력합니다\n","            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n","            print('Bot:', ' '.join(output_words))\n","\n","        except KeyError:\n","            print(\"Error: Encountered unknown word.\")"],"metadata":{"id":"5shtD51GrnbM","executionInfo":{"status":"ok","timestamp":1649247992684,"user_tz":-540,"elapsed":1,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# 모델을 설정합니다\n","model_name = 'cb_model'\n","attn_model = 'dot'\n","#attn_model = 'general'\n","#attn_model = 'concat'\n","hidden_size = 500\n","encoder_n_layers = 2\n","decoder_n_layers = 2\n","dropout = 0.1\n","batch_size = 64\n","\n","# 불러올 checkpoint를 설정합니다. 처음부터 시작할 때는 None으로 둡니다.\n","loadFilename = None\n","checkpoint_iter = 4000\n","#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n","#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n","#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n","\n","\n","# loadFilename이 제공되는 경우에는 모델을 불러옵니다\n","if loadFilename:\n","    # 모델을 학습할 때와 같은 기기에서 불러오는 경우\n","    checkpoint = torch.load(loadFilename)\n","    # GPU에서 학습한 모델을 CPU로 불러오는 경우\n","    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n","    encoder_sd = checkpoint['en']\n","    decoder_sd = checkpoint['de']\n","    encoder_optimizer_sd = checkpoint['en_opt']\n","    decoder_optimizer_sd = checkpoint['de_opt']\n","    embedding_sd = checkpoint['embedding']\n","    voc.__dict__ = checkpoint['voc_dict']\n","\n","\n","print('Building encoder and decoder ...')\n","# 단어 임베딩을 초기화합니다\n","embedding = nn.Embedding(voc.num_words, hidden_size)\n","if loadFilename:\n","    embedding.load_state_dict(embedding_sd)\n","# 인코더 및 디코더 모델을 초기화합니다\n","encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n","decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n","if loadFilename:\n","    encoder.load_state_dict(encoder_sd)\n","    decoder.load_state_dict(decoder_sd)\n","# 적절한 디바이스를 사용합니다\n","encoder = encoder.to(device)\n","decoder = decoder.to(device)\n","print('Models built and ready to go!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0DPd5JWKrouZ","outputId":"0551a46d-4bab-4c1b-9a99-67d6def54ec0","executionInfo":{"status":"ok","timestamp":1649247999658,"user_tz":-540,"elapsed":500,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Building encoder and decoder ...\n","Models built and ready to go!\n"]}]},{"cell_type":"code","source":["# 학습 및 최적화 설정\n","clip = 50.0\n","teacher_forcing_ratio = 1.0\n","learning_rate = 0.0001\n","decoder_learning_ratio = 5.0\n","n_iteration = 5000\n","print_every = 10\n","save_every = 1000\n","\n","# Dropout 레이어를 학습 모드로 둡니다\n","encoder.train()\n","decoder.train()\n","\n","# Optimizer를 초기화합니다\n","print('Building optimizers ...')\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n","if loadFilename:\n","    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n","    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n","\n","# cuda가 있다면 cuda를 설정합니다\n","for state in encoder_optimizer.state.values():\n","    for k, v in state.items():\n","        if isinstance(v, torch.Tensor):\n","            state[k] = v.cuda()\n","\n","for state in decoder_optimizer.state.values():\n","    for k, v in state.items():\n","        if isinstance(v, torch.Tensor):\n","            state[k] = v.cuda()\n","\n","# 학습 단계를 수행합니다\n","print(\"Starting Training!\")\n","trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n","           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n","           print_every, save_every, clip, corpus_name, loadFilename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"SbVVjJUsrqsP","outputId":"e09f5cc3-78b4-4b9a-fea1-c7685c784b1f","executionInfo":{"status":"error","timestamp":1649250497009,"user_tz":-540,"elapsed":2475489,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Building optimizers ...\n","Starting Training!\n","Initializing ...\n","Training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1356.)\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:156: UserWarning: masked_scatter_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1765.)\n","  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"]},{"output_type":"stream","name":"stdout","text":["Iteration: 10; Percent complete: 0.2%; Average loss: 7.9138\n","Iteration: 20; Percent complete: 0.4%; Average loss: 6.0401\n","Iteration: 30; Percent complete: 0.6%; Average loss: 5.5387\n","Iteration: 40; Percent complete: 0.8%; Average loss: 5.4048\n","Iteration: 50; Percent complete: 1.0%; Average loss: 5.3398\n","Iteration: 60; Percent complete: 1.2%; Average loss: 5.2573\n","Iteration: 70; Percent complete: 1.4%; Average loss: 5.2466\n","Iteration: 80; Percent complete: 1.6%; Average loss: 5.1226\n","Iteration: 90; Percent complete: 1.8%; Average loss: 5.1216\n","Iteration: 100; Percent complete: 2.0%; Average loss: 4.9752\n","Iteration: 110; Percent complete: 2.2%; Average loss: 5.0606\n","Iteration: 120; Percent complete: 2.4%; Average loss: 5.0647\n","Iteration: 130; Percent complete: 2.6%; Average loss: 4.9553\n","Iteration: 140; Percent complete: 2.8%; Average loss: 4.8687\n","Iteration: 150; Percent complete: 3.0%; Average loss: 4.8512\n","Iteration: 160; Percent complete: 3.2%; Average loss: 4.6665\n","Iteration: 170; Percent complete: 3.4%; Average loss: 4.6880\n","Iteration: 180; Percent complete: 3.6%; Average loss: 4.6605\n","Iteration: 190; Percent complete: 3.8%; Average loss: 4.6644\n","Iteration: 200; Percent complete: 4.0%; Average loss: 4.6203\n","Iteration: 210; Percent complete: 4.2%; Average loss: 4.6000\n","Iteration: 220; Percent complete: 4.4%; Average loss: 4.6110\n","Iteration: 230; Percent complete: 4.6%; Average loss: 4.5126\n","Iteration: 240; Percent complete: 4.8%; Average loss: 4.4498\n","Iteration: 250; Percent complete: 5.0%; Average loss: 4.4455\n","Iteration: 260; Percent complete: 5.2%; Average loss: 4.4036\n","Iteration: 270; Percent complete: 5.4%; Average loss: 4.3704\n","Iteration: 280; Percent complete: 5.6%; Average loss: 4.2543\n","Iteration: 290; Percent complete: 5.8%; Average loss: 4.2586\n","Iteration: 300; Percent complete: 6.0%; Average loss: 4.2626\n","Iteration: 310; Percent complete: 6.2%; Average loss: 4.1967\n","Iteration: 320; Percent complete: 6.4%; Average loss: 4.1256\n","Iteration: 330; Percent complete: 6.6%; Average loss: 4.1368\n","Iteration: 340; Percent complete: 6.8%; Average loss: 4.0896\n","Iteration: 350; Percent complete: 7.0%; Average loss: 4.0490\n","Iteration: 360; Percent complete: 7.2%; Average loss: 4.0210\n","Iteration: 370; Percent complete: 7.4%; Average loss: 3.8973\n","Iteration: 380; Percent complete: 7.6%; Average loss: 3.8487\n","Iteration: 390; Percent complete: 7.8%; Average loss: 3.8761\n","Iteration: 400; Percent complete: 8.0%; Average loss: 3.7886\n","Iteration: 410; Percent complete: 8.2%; Average loss: 3.7329\n","Iteration: 420; Percent complete: 8.4%; Average loss: 3.7247\n","Iteration: 430; Percent complete: 8.6%; Average loss: 3.6035\n","Iteration: 440; Percent complete: 8.8%; Average loss: 3.5798\n","Iteration: 450; Percent complete: 9.0%; Average loss: 3.5518\n","Iteration: 460; Percent complete: 9.2%; Average loss: 3.5016\n","Iteration: 470; Percent complete: 9.4%; Average loss: 3.4504\n","Iteration: 480; Percent complete: 9.6%; Average loss: 3.4877\n","Iteration: 490; Percent complete: 9.8%; Average loss: 3.3700\n","Iteration: 500; Percent complete: 10.0%; Average loss: 3.2971\n","Iteration: 510; Percent complete: 10.2%; Average loss: 3.2530\n","Iteration: 520; Percent complete: 10.4%; Average loss: 3.0656\n","Iteration: 530; Percent complete: 10.6%; Average loss: 3.1256\n","Iteration: 540; Percent complete: 10.8%; Average loss: 3.0390\n","Iteration: 550; Percent complete: 11.0%; Average loss: 3.0329\n","Iteration: 560; Percent complete: 11.2%; Average loss: 3.0366\n","Iteration: 570; Percent complete: 11.4%; Average loss: 2.8875\n","Iteration: 580; Percent complete: 11.6%; Average loss: 2.8247\n","Iteration: 590; Percent complete: 11.8%; Average loss: 2.8162\n","Iteration: 600; Percent complete: 12.0%; Average loss: 2.7018\n","Iteration: 610; Percent complete: 12.2%; Average loss: 2.6202\n","Iteration: 620; Percent complete: 12.4%; Average loss: 2.6142\n","Iteration: 630; Percent complete: 12.6%; Average loss: 2.6347\n","Iteration: 640; Percent complete: 12.8%; Average loss: 2.4905\n","Iteration: 650; Percent complete: 13.0%; Average loss: 2.4195\n","Iteration: 660; Percent complete: 13.2%; Average loss: 2.4160\n","Iteration: 670; Percent complete: 13.4%; Average loss: 2.3189\n","Iteration: 680; Percent complete: 13.6%; Average loss: 2.2078\n","Iteration: 690; Percent complete: 13.8%; Average loss: 2.2232\n","Iteration: 700; Percent complete: 14.0%; Average loss: 2.1598\n","Iteration: 710; Percent complete: 14.2%; Average loss: 2.1559\n","Iteration: 720; Percent complete: 14.4%; Average loss: 1.8962\n","Iteration: 730; Percent complete: 14.6%; Average loss: 1.9219\n","Iteration: 740; Percent complete: 14.8%; Average loss: 1.9197\n","Iteration: 750; Percent complete: 15.0%; Average loss: 1.8249\n","Iteration: 760; Percent complete: 15.2%; Average loss: 1.7891\n","Iteration: 770; Percent complete: 15.4%; Average loss: 1.8163\n","Iteration: 780; Percent complete: 15.6%; Average loss: 1.6993\n","Iteration: 790; Percent complete: 15.8%; Average loss: 1.6732\n","Iteration: 800; Percent complete: 16.0%; Average loss: 1.5829\n","Iteration: 810; Percent complete: 16.2%; Average loss: 1.5079\n","Iteration: 820; Percent complete: 16.4%; Average loss: 1.5496\n","Iteration: 830; Percent complete: 16.6%; Average loss: 1.4259\n","Iteration: 840; Percent complete: 16.8%; Average loss: 1.4676\n","Iteration: 850; Percent complete: 17.0%; Average loss: 1.3879\n","Iteration: 860; Percent complete: 17.2%; Average loss: 1.3311\n","Iteration: 870; Percent complete: 17.4%; Average loss: 1.2840\n","Iteration: 880; Percent complete: 17.6%; Average loss: 1.2177\n","Iteration: 890; Percent complete: 17.8%; Average loss: 1.1487\n","Iteration: 900; Percent complete: 18.0%; Average loss: 1.1517\n","Iteration: 910; Percent complete: 18.2%; Average loss: 1.0900\n","Iteration: 920; Percent complete: 18.4%; Average loss: 1.0297\n","Iteration: 930; Percent complete: 18.6%; Average loss: 1.0679\n","Iteration: 940; Percent complete: 18.8%; Average loss: 0.9655\n","Iteration: 950; Percent complete: 19.0%; Average loss: 0.9903\n","Iteration: 960; Percent complete: 19.2%; Average loss: 0.9792\n","Iteration: 970; Percent complete: 19.4%; Average loss: 0.8742\n","Iteration: 980; Percent complete: 19.6%; Average loss: 0.8469\n","Iteration: 990; Percent complete: 19.8%; Average loss: 0.8149\n","Iteration: 1000; Percent complete: 20.0%; Average loss: 0.8147\n","Iteration: 1010; Percent complete: 20.2%; Average loss: 0.7391\n","Iteration: 1020; Percent complete: 20.4%; Average loss: 0.7404\n","Iteration: 1030; Percent complete: 20.6%; Average loss: 0.7229\n","Iteration: 1040; Percent complete: 20.8%; Average loss: 0.6810\n","Iteration: 1050; Percent complete: 21.0%; Average loss: 0.6917\n","Iteration: 1060; Percent complete: 21.2%; Average loss: 0.6444\n","Iteration: 1070; Percent complete: 21.4%; Average loss: 0.5765\n","Iteration: 1080; Percent complete: 21.6%; Average loss: 0.6257\n","Iteration: 1090; Percent complete: 21.8%; Average loss: 0.5881\n","Iteration: 1100; Percent complete: 22.0%; Average loss: 0.5672\n","Iteration: 1110; Percent complete: 22.2%; Average loss: 0.5461\n","Iteration: 1120; Percent complete: 22.4%; Average loss: 0.4905\n","Iteration: 1130; Percent complete: 22.6%; Average loss: 0.5217\n","Iteration: 1140; Percent complete: 22.8%; Average loss: 0.5134\n","Iteration: 1150; Percent complete: 23.0%; Average loss: 0.4651\n","Iteration: 1160; Percent complete: 23.2%; Average loss: 0.4440\n","Iteration: 1170; Percent complete: 23.4%; Average loss: 0.3986\n","Iteration: 1180; Percent complete: 23.6%; Average loss: 0.4441\n","Iteration: 1190; Percent complete: 23.8%; Average loss: 0.3477\n","Iteration: 1200; Percent complete: 24.0%; Average loss: 0.3280\n","Iteration: 1210; Percent complete: 24.2%; Average loss: 0.3393\n","Iteration: 1220; Percent complete: 24.4%; Average loss: 0.3050\n","Iteration: 1230; Percent complete: 24.6%; Average loss: 0.3217\n","Iteration: 1240; Percent complete: 24.8%; Average loss: 0.3112\n","Iteration: 1250; Percent complete: 25.0%; Average loss: 0.3219\n","Iteration: 1260; Percent complete: 25.2%; Average loss: 0.2929\n","Iteration: 1270; Percent complete: 25.4%; Average loss: 0.3069\n","Iteration: 1280; Percent complete: 25.6%; Average loss: 0.2895\n","Iteration: 1290; Percent complete: 25.8%; Average loss: 0.2604\n","Iteration: 1300; Percent complete: 26.0%; Average loss: 0.2854\n","Iteration: 1310; Percent complete: 26.2%; Average loss: 0.2658\n","Iteration: 1320; Percent complete: 26.4%; Average loss: 0.2578\n","Iteration: 1330; Percent complete: 26.6%; Average loss: 0.2271\n","Iteration: 1340; Percent complete: 26.8%; Average loss: 0.2225\n","Iteration: 1350; Percent complete: 27.0%; Average loss: 0.2270\n","Iteration: 1360; Percent complete: 27.2%; Average loss: 0.2160\n","Iteration: 1370; Percent complete: 27.4%; Average loss: 0.2168\n","Iteration: 1380; Percent complete: 27.6%; Average loss: 0.2009\n","Iteration: 1390; Percent complete: 27.8%; Average loss: 0.1751\n","Iteration: 1400; Percent complete: 28.0%; Average loss: 0.1956\n","Iteration: 1410; Percent complete: 28.2%; Average loss: 0.1857\n","Iteration: 1420; Percent complete: 28.4%; Average loss: 0.1722\n","Iteration: 1430; Percent complete: 28.6%; Average loss: 0.1546\n","Iteration: 1440; Percent complete: 28.8%; Average loss: 0.1781\n","Iteration: 1450; Percent complete: 29.0%; Average loss: 0.1662\n","Iteration: 1460; Percent complete: 29.2%; Average loss: 0.1438\n","Iteration: 1470; Percent complete: 29.4%; Average loss: 0.1586\n","Iteration: 1480; Percent complete: 29.6%; Average loss: 0.1404\n","Iteration: 1490; Percent complete: 29.8%; Average loss: 0.1327\n","Iteration: 1500; Percent complete: 30.0%; Average loss: 0.1331\n","Iteration: 1510; Percent complete: 30.2%; Average loss: 0.1318\n","Iteration: 1520; Percent complete: 30.4%; Average loss: 0.1339\n","Iteration: 1530; Percent complete: 30.6%; Average loss: 0.1027\n","Iteration: 1540; Percent complete: 30.8%; Average loss: 0.1181\n","Iteration: 1550; Percent complete: 31.0%; Average loss: 0.1064\n","Iteration: 1560; Percent complete: 31.2%; Average loss: 0.1042\n","Iteration: 1570; Percent complete: 31.4%; Average loss: 0.0964\n","Iteration: 1580; Percent complete: 31.6%; Average loss: 0.1039\n","Iteration: 1590; Percent complete: 31.8%; Average loss: 0.0955\n","Iteration: 1600; Percent complete: 32.0%; Average loss: 0.0936\n","Iteration: 1610; Percent complete: 32.2%; Average loss: 0.1014\n","Iteration: 1620; Percent complete: 32.4%; Average loss: 0.0956\n","Iteration: 1630; Percent complete: 32.6%; Average loss: 0.0889\n","Iteration: 1640; Percent complete: 32.8%; Average loss: 0.0909\n","Iteration: 1650; Percent complete: 33.0%; Average loss: 0.0840\n","Iteration: 1660; Percent complete: 33.2%; Average loss: 0.0818\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-fac29c84c3a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n\u001b[1;32m     36\u001b[0m            \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_n_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_n_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m            print_every, save_every, clip, corpus_name, loadFilename)\n\u001b[0m","\u001b[0;32m<ipython-input-40-3ec13c269d8d>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# 배치에 대해 학습을 한 단계 진행합니다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n\u001b[0;32m---> 23\u001b[0;31m                      decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-39-9239f00612f1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# 모델의 가중치를 수정합니다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# Dropout 레이어를 평가 모드로 설정합니다\n","encoder.eval()\n","decoder.eval()\n","\n","# 탐색 모듈을 초기화합니다\n","searcher = GreedySearchDecoder(encoder, decoder)\n","\n","# 채팅을 시작합니다 \n","evaluateInput(encoder, decoder, searcher, voc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uN7Uu0fprrQg","outputId":"10fd1589-5358-4546-bd6d-9c3be046f43e","executionInfo":{"status":"ok","timestamp":1649251594016,"user_tz":-540,"elapsed":1092998,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}}},"execution_count":45,"outputs":[{"name":"stdout","output_type":"stream","text":["> 안녕?\n","Bot: 안녕하세요 . . . . .\n","> 밥 먹었니?\n","Bot: 배고프지 않아요 . . . .\n","> 나는 배고파\n","Bot: 맛난 먹고 해보는 게 좋을 것 같아요 . .\n","> 과일 먹고 잘까\n","Error: Encountered unknown word.\n","> 과일 먹고 자야지\n","Bot: 제철과일이 정말 좋아요 . . . .\n","> 김치볶음밥 먹을래\n","Error: Encountered unknown word.\n","> 김치볶음밥 먹어야지\n","Error: Encountered unknown word.\n","> 김치찌개 먹고싶어\n","Error: Encountered unknown word.\n","> 나 혼자 좋아해\n","Bot: 혼자 사는 것도 나쁘지 않아요 . . .\n","> 마음이 너무 아파\n","Bot: 아픈 만큼 행복해지시길 바랄게요 . . .\n","> 연애가 필요해\n","Bot: 제가 있잖아요 . . . .\n","> 마음이 불안해\n","Bot: 인연이 거기까지인가봐요 . . . .\n","> 불안해\n","Bot: 두려워하지 않아도 돼요 . . . .\n","> 계속 아프면 어떡하지\n","Bot: 일찍 주무세요 . ! ! !\n","> 숨이 너무 막혀\n","Error: Encountered unknown word.\n","> 여행이나 가고싶어\n","Error: Encountered unknown word.\n","> 여행 가고싶어\n","Error: Encountered unknown word.\n","> 여행이나 갈래\n","Error: Encountered unknown word.\n","> 여행 갈래\n","Error: Encountered unknown word.\n","> 여행\n","Bot: 편하게 쉴 수 있는 곳이 좋을 거예요 . .\n","> 허리가 너무 아파\n","Bot: 스트레칭을 해보세요 . . . .\n","> 계속 아프면 어떡하지\n","Bot: 일찍 주무세요 . ! ! !\n","> 불안해\n","Bot: 두려워하지 않아도 돼요 . . . .\n","> 속이 안좋아\n","Error: Encountered unknown word.\n","> 약 먹기 싫어\n","Bot: 그냥 쓴 보세요 . 하지마세요 . .\n","> 약 먹어도 아파\n","Bot: 두통약 드세요 . . . .\n","> 신경써서 머리도 아파\n","Error: Encountered unknown word.\n","> 이제는 머리도 아파\n","Error: Encountered unknown word.\n","> q\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"Nn1uo1DZv46n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MFsckb_YHHG4","executionInfo":{"status":"ok","timestamp":1649239897077,"user_tz":-540,"elapsed":15009,"user":{"displayName":"이승연(일반대학원 신소재공학과)","userId":"05029987096427902128"}},"outputId":"cce740ac-1eb2-4626-b99a-081815c7c5d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"rNf1WO8FHHlg"},"execution_count":null,"outputs":[]}]}